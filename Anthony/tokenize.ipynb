{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize captions and get embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping bad lines - return to this later\n",
      "(155392, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>above_text</th>\n",
       "      <th>below_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54053</th>\n",
       "      <td>My Precious Gollum</td>\n",
       "      <td>Hello TARA...</td>\n",
       "      <td>HELLO PRECIOUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67160</th>\n",
       "      <td>Ecstatic Michael Phelps</td>\n",
       "      <td>tHERE'S A POT OF THE STUFF?</td>\n",
       "      <td>i LOVE POT.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49626</th>\n",
       "      <td>katt williams shocked</td>\n",
       "      <td>What</td>\n",
       "      <td>you actually thought you were getting rp?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>Okay Guy</td>\n",
       "      <td>TOOK AN ARROW TO THE KNEE</td>\n",
       "      <td>OKAY..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22047</th>\n",
       "      <td>Rich Men Laughing</td>\n",
       "      <td>and then we told them</td>\n",
       "      <td>their health insurance premiums wouldnt go up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150193</th>\n",
       "      <td>kim jong un</td>\n",
       "      <td>they see me rulin'</td>\n",
       "      <td>they hatin'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121915</th>\n",
       "      <td>The Olympic Queen</td>\n",
       "      <td>vodka</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132328</th>\n",
       "      <td>Paperclip</td>\n",
       "      <td>it looks like you're having trouble</td>\n",
       "      <td>fapping to this meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120444</th>\n",
       "      <td>Honey BooBoo</td>\n",
       "      <td>happy birthday</td>\n",
       "      <td>ali boo boo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15655</th>\n",
       "      <td>Not sure if troll</td>\n",
       "      <td>not sure if nicki minaj</td>\n",
       "      <td>or a mutant from mortal combat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image                           above_text  \\\n",
       "54053        My Precious Gollum                        Hello TARA...   \n",
       "67160   Ecstatic Michael Phelps          tHERE'S A POT OF THE STUFF?   \n",
       "49626     katt williams shocked                                 What   \n",
       "9727                   Okay Guy            TOOK AN ARROW TO THE KNEE   \n",
       "22047         Rich Men Laughing                and then we told them   \n",
       "150193              kim jong un                   they see me rulin'   \n",
       "121915        The Olympic Queen                                vodka   \n",
       "132328                Paperclip  it looks like you're having trouble   \n",
       "120444             Honey BooBoo                       happy birthday   \n",
       "15655         Not sure if troll              not sure if nicki minaj   \n",
       "\n",
       "                                           below_text  \n",
       "54053                                  HELLO PRECIOUS  \n",
       "67160                                     i LOVE POT.  \n",
       "49626       you actually thought you were getting rp?  \n",
       "9727                                           OKAY..  \n",
       "22047   their health insurance premiums wouldnt go up  \n",
       "150193                                    they hatin'  \n",
       "121915                                            NaN  \n",
       "132328                           fapping to this meme  \n",
       "120444                                    ali boo boo  \n",
       "15655                  or a mutant from mortal combat  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = pd.read_csv(\"captions.csv\", error_bad_lines=False, warn_bad_lines=False)\n",
    "print(\"Skipping bad lines - return to this later\")\n",
    "print(captions.shape)\n",
    "captions.sample(10, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a non-negligible number of captions written in Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image           13\n",
       "above_text    6137\n",
       "below_text    7199\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pd.isna(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>above_text</th>\n",
       "      <th>below_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18546</th>\n",
       "      <td>NaN</td>\n",
       "      <td>several people get up and leave as they can se...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43899</th>\n",
       "      <td>NaN</td>\n",
       "      <td>teacher is even later than you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57525</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Ekki málið :)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100719</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Ert þú starfsmaður þarna eða eigandi?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100723</th>\n",
       "      <td>NaN</td>\n",
       "      <td>uppiskorpi!!!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100725</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Eða bara eldisfiskur. LOL.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100728</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Takk kærlega fyrir þetta :)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105241</th>\n",
       "      <td>NaN</td>\n",
       "      <td>makes us strong</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105243</th>\n",
       "      <td>NaN</td>\n",
       "      <td>makes us strong</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114690</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Nei þá nærðu í rauðvín</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114721</th>\n",
       "      <td>NaN</td>\n",
       "      <td>merkileg blanda alveg;)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115828</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fannst þetta bara krúttleg mynd =)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132127</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I NEVER ASK FOR THIS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image                                         above_text below_text\n",
       "18546    NaN  several people get up and leave as they can se...        NaN\n",
       "43899    NaN                     teacher is even later than you        NaN\n",
       "57525    NaN                                      Ekki málið :)        NaN\n",
       "100719   NaN              Ert þú starfsmaður þarna eða eigandi?        NaN\n",
       "100723   NaN                                      uppiskorpi!!!        NaN\n",
       "100725   NaN                         Eða bara eldisfiskur. LOL.        NaN\n",
       "100728   NaN                        Takk kærlega fyrir þetta :)        NaN\n",
       "105241   NaN                                    makes us strong        NaN\n",
       "105243   NaN                                    makes us strong        NaN\n",
       "114690   NaN                             Nei þá nærðu í rauðvín        NaN\n",
       "114721   NaN                            merkileg blanda alveg;)        NaN\n",
       "115828   NaN                 Fannst þetta bara krúttleg mynd =)        NaN\n",
       "132127   NaN                               I NEVER ASK FOR THIS        NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.iloc[np.where(pd.isna(captions.image))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NA values for labels appear to happen when text is in a different language. I think it is safe to say that we can drop these. For `above_text` and `below_text`, this indicates that the meme did not contain text either above or below the picture. We can't throw these out, so just replace them with a empty string.\n",
    "\n",
    "**IS THIS THE RIGHT THING TO DO?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = captions[pd.notnull(captions.image)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = captions.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image         0\n",
       "above_text    0\n",
       "below_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pd.isna(captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up vocabulary dictionary\n",
    "\n",
    "In Dank Learning, it looks like they create a vocabulary dictionary from all words in the captions and labels, i.e., meme format names. See [here](https://github.com/alpv95/MemeProject/blob/master/im2txt/MemeNote.ipynb) for their exact process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phrases = np.append(captions.image, [captions.above_text, captions.below_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its all over youtube\n",
      "Progressive Guitarist\n",
      "this is madness\n",
      "i like the way you flatshot that .25\" plate.\n",
      "Oh you think war is your ally? But you merely adopted the Uniwar.\n",
      "i'm going to make a ton of threads on /b/\n",
      "Advice Polack\n",
      "That would make me soooo happy.\n",
      "your people is just to stupid\n",
      "#HASHTAGS EVERYWHERE\n"
     ]
    }
   ],
   "source": [
    "rand_inds = np.random.randint(len(all_phrases)-1, size=10)\n",
    "for phrase in all_phrases[rand_inds]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[\\w\\']+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just look at all (unique) words to inspect if anything looks wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for phrase in all_phrases:\n",
    "    for word in tokenizer.tokenize(phrase):\n",
    "        all_words.append(word)\n",
    "unique_words = list(set(all_words))\n",
    "all_words.sort()\n",
    "unique_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 1845822\n",
      "Number of unique words: 117496\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words:\", len(all_words))\n",
    "print(\"Number of unique words:\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict([(y,x) for x,y in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\", \"''\", \"'''angry\", \"''50\", \"''BFF's''\", \"''Busy\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting empty strings in weird places... that seems off\n",
    "list(word2idx.keys())[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize each caption and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tokens_label = []\n",
    "for label in captions.image:\n",
    "    list_of_tokens_label.append(tokenizer.tokenize(label))\n",
    "    \n",
    "list_of_tokens_above_text = []\n",
    "for above_text in captions.above_text:\n",
    "    list_of_tokens_above_text.append(tokenizer.tokenize(above_text))\n",
    "    \n",
    "list_of_tokens_below_text = []\n",
    "for below_text in captions.below_text:\n",
    "    list_of_tokens_below_text.append(tokenizer.tokenize(below_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access GloVe pre-trained vectors from [Standford's Github repo](https://github.com/stanfordnlp/GloVe). This is the Common Crawl one with 42 billion tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 613311  words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove_model = loadGloveModel(\"glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now doing this on a sample of 10\n",
    "sample_inds = [223, 51788, 2112, 777, 1120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = []\n",
    "real_above_text = []\n",
    "real_below_text = []\n",
    "label_embeddings = []\n",
    "above_text_embeddings = []\n",
    "below_text_embeddings = []\n",
    "for si in sample_inds:\n",
    "    label_embeddings.append([glove_model.get(elem) for elem in list_of_tokens_label[si]])\n",
    "    above_text_embeddings.append([glove_model.get(elem) for elem in list_of_tokens_above_text[si]])\n",
    "    below_text_embeddings.append([glove_model.get(elem) for elem in list_of_tokens_below_text[si]])\n",
    "    real_label.append(list_of_tokens_label[si])\n",
    "    real_above_text.append(list_of_tokens_above_text[si])\n",
    "    real_below_text.append(list_of_tokens_below_text[si])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tuple = (real_label, real_above_text, real_below_text, label_embeddings, above_text_embeddings, below_text_embeddings)\n",
    "with open(\"sample_data\", \"wb\") as f:\n",
    "    pickle.dump(sample_tuple, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Spanish words\n",
    "\n",
    "When using GloVe embeddings, we'll map words to unknown tag and we can naively remove all the unknown tags (as some of these will likely correspond to Spanish words). This means remove all captions with unknown tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
