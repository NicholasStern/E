{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, RepeatVector, Activation, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "os.chdir('../Anthony/')\n",
    "from pickle_utils import pickle_load, pickle_dump\n",
    "os.chdir('../lipika/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding shit\n",
    "glove_index_dict, embeddings = pickle_load(\"../Anthony/glove_objs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "img_shape = (300,300,3)\n",
    "vocab_size = embeddings.shape[0]\n",
    "embedding_size = 300\n",
    "maxlen = 20                         # maximum length of the caption in hidden state\n",
    "\n",
    "hidden_units = embedding_size       # length of word vectors i.e. embedding size\n",
    "\n",
    "# hyper params\n",
    "clip_norm = 1.0\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_model(img_shape,\n",
    "                  vocab_size, \n",
    "                  embedding_size, \n",
    "                  maxlen, \n",
    "                  hidden_units, \n",
    "                  clip_norm\n",
    "                  learning_rate):\n",
    "    \n",
    "    # =============\n",
    "    #   ENCODER\n",
    "    # =============\n",
    "    # Inputs: \n",
    "    #     1. Image (300, 300, 3)\n",
    "    #     2. GloVe-ed label embeddings (300,)\n",
    "\n",
    "    # 1. Image Input\n",
    "    input_img = keras.Input(shape=img_shape)\n",
    "    # 2. Label Embedding\n",
    "    label_emb = keras.Input(shape=(300,))\n",
    "\n",
    "    # Model:\n",
    "    #     1. Pretrained CNN with classification layer peeled off\n",
    "    #           - Output size: (2048,)\n",
    "    #     2. Concatenate with label embeding of size\n",
    "    #           -  Output size: (2348,)\n",
    "    #     3. MLP: Dense layer with 300 nodes\n",
    "    #           - Output size: (300,) <-- This is the image embedding\n",
    "    \n",
    "    \n",
    "    # 1. Define Pretrained CNN - Inception V3\n",
    "    cnnModel = InceptionV3(weights='imagenet', \n",
    "                           include_top=False,        # this removes the final layer\n",
    "                           input_shape=img_shape, \n",
    "                           pooling = 'avg')\n",
    "\n",
    "    # freeze all convolutional InceptionV3 layers\n",
    "    for layer in cnnModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get image embedding <- this is a model output\n",
    "    image_emb = cnnModel(input_img)\n",
    "\n",
    "    # 2. Concatenate image embedding with label embeding of size\n",
    "    concat = keras.layers.Concatenate(axis=1)([image_emb, label_emb])\n",
    "    \n",
    "    # 3. MLP with 300 nodes\n",
    "    full_img_embedding = Dense(300, activation='relu')(concat)\n",
    "\n",
    "    # Attention Implementation\n",
    "    #     1. MLP with 300 nodes (so weights can be learned) w *softmax* activation to get importance probability\n",
    "    #     2. Multiply output of the full_img_embedding layer (Model part 3.) with \"probabilities\"\n",
    "    \n",
    "    # 1. MLP with 300 nodes w softmax\n",
    "    softmax_encoder = Dense(300,activation = 'softmax', name = 'softmax_encoder')(full_img_embedding)\n",
    "    \n",
    "    # 2. Multiply layer i.e. probability weighted vector for naive impl of attention\n",
    "    attention_encoder = keras.layers.Multiply()([totalEmbeddingLayer, softmax_encoder])\n",
    "\n",
    "    # ==== ENCODER MODEL ====\n",
    "    encoder = keras.Model(inputs=[input_img, label_emb], outputs=attention_encoder)\n",
    "\n",
    "    # =============\n",
    "    #   DECODER\n",
    "    # =============\n",
    "    # Inputs: \n",
    "    #     1. Caption (tokenized) (20,) <- limiting caption length to 20\n",
    "    #     2. LSTM hidden state from encoder\n",
    "    \n",
    "    # 1. Caption\n",
    "    input_caption = keras.Input(shape = (maxlen,))\n",
    "    \n",
    "    # 2. Input for the LSTM hidden state and/or cell state\n",
    "    initial_state_LSTM = encoder([input_img, label_emb])\n",
    "    \n",
    "    # Model:\n",
    "    #     1. Embedding layer that uses the GloVe embedding matrix, and is set to be trainable\n",
    "    #           - Output size: (20, 300)\n",
    "    #     2. LSTM\n",
    "    #           -  Output size: (20, 300)\n",
    "    #     3. Time Distributed layer to apply Dense layer to all the time step outputs\n",
    "    #           - Output size: (20, 40000)\n",
    "    #     4. Activation of softmax to get values between 0 and 1\n",
    "    #           - Output size: (20, 40000)\n",
    "        \n",
    "    # 1. Embedding layer\n",
    "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_size,\n",
    "                                  input_length=maxlend, W_regularizer = None,\n",
    "                                  weights = [embeddings], name = 'caption_embeddings', \n",
    "                                  trainable = True)\n",
    "    # 2. LSTM\n",
    "    decoder_LSTM = LSTM(hidden_units,return_sequences=True, return_state=True)\n",
    "\n",
    "    ## ===== Get embedding and LSTM outputs =====\n",
    "    decoder_embedding_outputs = decoder_embedding(decoder_inputs)\n",
    "    decoder_LSTM_outputs, _ , _ = decoder_LSTM(decoder_embedding_outputs, \n",
    "                                          initial_state = [initial_state_LSTM,  # hidden state\n",
    "                                                           initial_state_LSTM]) # cell state\n",
    "    \n",
    "    # 3. Time Distributed Layer\n",
    "    time_distributed = TimeDistributed(Dense(vocab_size, name = 'timedistributed_1'))\n",
    "    \n",
    "    # 4. Softmax \n",
    "    activation = Activation('softmax')\n",
    "    \n",
    "    ## ===== Get time distributed and softmax output =====\n",
    "    time_distributed_output = time_distributed(decoder_outputs)\n",
    "    decoder_outputs = activation(time_distributed_output)\n",
    "\n",
    "    # ==============\n",
    "    #   FULL MODEL\n",
    "    # ==============   \n",
    "    model= Model(inputs=[input_img, label_emb, input_caption], outputs=decoder_outputs)\n",
    "    rmsprop = RMSprop(lr=learning_rate,clipnorm=clip_norm)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=rmsprop)\n",
    "\n",
    "    return encoder, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
