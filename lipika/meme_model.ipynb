{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meme Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, RepeatVector, Activation, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "from pickle_utils import pickle_load, pickle_dump\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "embeddings, idx2word, word2idx, captions = pickle_load(\"small_processed_data.pkl\")\n",
    "captions.image = captions.image.apply(lambda x: x.strip(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "img_shape = (300,300,3)\n",
    "vocab_size = embeddings.shape[0]\n",
    "embedding_size = 300\n",
    "maxlen = 30                         # maximum length of the caption in hidden state\n",
    "batch_size = 32\n",
    "hidden_units = embedding_size       # length of word vectors i.e. embedding size\n",
    "\n",
    "# hyper params\n",
    "clip_norm = 1.0\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(img_shape, vocab_size, embedding_size, maxlen, hidden_units, clip_norm, learning_rate):\n",
    "    '''\n",
    "    =============\n",
    "      ENCODER\n",
    "    =============\n",
    "    Inputs: \n",
    "        1. Image (300, 300, 3)\n",
    "        2. GloVe-ed label embeddings (300,)\n",
    "        \n",
    "    Model:\n",
    "        3. Pretrained CNN with classification layer peeled off\n",
    "              - Output size: (2048,)\n",
    "        4. Concatenate with label embeding of size\n",
    "              - Output size: (2348,)\n",
    "        5. MLP: Dense layer with 300 nodes\n",
    "              - Output size: (300,) <-- This is the image embedding\n",
    "    '''\n",
    "\n",
    "    # 1. Image Input\n",
    "    input_img = keras.Input(shape=img_shape, name='image_input')\n",
    "    \n",
    "    # 2. Label Embedding\n",
    "    label_emb = keras.Input(shape=(300,), name='image_label_input')\n",
    "\n",
    "    # 3. Define Pretrained CNN - Inception V3\n",
    "    cnnModel = InceptionV3(weights='imagenet', \n",
    "                           include_top=False,        # this removes the final dense layer\n",
    "                           input_shape=img_shape, \n",
    "                           pooling = 'avg')\n",
    "\n",
    "    # freeze all convolutional InceptionV3 layers\n",
    "    for layer in cnnModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get image embedding <- this is a model output\n",
    "    image_emb = cnnModel(input_img)\n",
    "\n",
    "    # 4. Concatenate image embedding with label embedding\n",
    "    concat = keras.layers.Concatenate(axis=1)([image_emb, label_emb])\n",
    "    \n",
    "    # 5. MLP with 300 nodes\n",
    "    full_img_embedding = Dense(300, activation='relu')(concat)\n",
    "    \n",
    "    '''\n",
    "    =============\n",
    "    ATTENTION IMPLEMENTATION\n",
    "    =============\n",
    "        6. MLP with 300 nodes (so weights can be learned) w softmax activation to get importance probabilities\n",
    "        7. Multiply output of the full_img_embedding layer (Model part 5.) with \"probabilities\"\n",
    "    '''\n",
    "    \n",
    "    # 6. MLP with 300 nodes w softmax\n",
    "    softmax_encoder = Dense(300, activation = 'softmax', name = 'softmax_encoder')(full_img_embedding)\n",
    "    \n",
    "    # 7. Multiply layer i.e. probability weighted vector for naive impl of attention\n",
    "    attention_encoder = keras.layers.Multiply()([full_img_embedding, softmax_encoder])\n",
    "\n",
    "    # ==== ENCODER MODEL ====\n",
    "    encoder = keras.Model(inputs=[input_img, label_emb], outputs=attention_encoder)\n",
    "\n",
    "    '''\n",
    "    =============\n",
    "      DECODER\n",
    "    =============\n",
    "    Inputs: \n",
    "        8. Caption (tokenized) (20,) <- limiting caption length to 20\n",
    "        9. LSTM hidden state from encoder\n",
    "        \n",
    "    Model:\n",
    "        10. Embedding layer that uses the GloVe embedding matrix, and is set to be trainable\n",
    "              - Output size: (20, 300)\n",
    "        11. LSTM\n",
    "              -  Output size: (20, 300)\n",
    "        12. Time Distributed layer to apply Dense layer to all the time step outputs\n",
    "              - Output size: (20, 40000)\n",
    "        13. Activation of softmax to get values between 0 and 1\n",
    "              - Output size: (20, 40000)\n",
    "    '''\n",
    "\n",
    "    # 8. Caption\n",
    "    input_caption = keras.Input(shape = (maxlen,), name='image_caption_input')\n",
    "    \n",
    "    # 9. Input for the LSTM hidden state and/or cell state\n",
    "    initial_state_LSTM = encoder([input_img, label_emb])\n",
    "    \n",
    "\n",
    "        \n",
    "    # 10. Embedding layer\n",
    "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_size,\n",
    "                                  input_length=maxlen, embeddings_regularizer = None,\n",
    "                                  weights = [embeddings], name = 'caption_embeddings', \n",
    "                                  trainable = True, mask_zero=True)\n",
    "    # 11. LSTM\n",
    "    decoder_LSTM = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    ## ===== Get embedding and LSTM outputs =====\n",
    "    decoder_embedding_outputs = decoder_embedding(input_caption)\n",
    "    decoder_LSTM_outputs, _ , _ = decoder_LSTM(decoder_embedding_outputs, \n",
    "                                          initial_state = [initial_state_LSTM,  # hidden state\n",
    "                                                           initial_state_LSTM]) # cell state\n",
    "    \n",
    "    # 12. Time Distributed Layer\n",
    "    time_distributed = TimeDistributed(Dense(vocab_size, name = 'timedistributed_1'))\n",
    "    \n",
    "    # 13. Softmax \n",
    "    activation = Activation('softmax')\n",
    "    \n",
    "    ## ===== Get time distributed and softmax output =====\n",
    "    time_distributed_output = time_distributed(decoder_LSTM_outputs)\n",
    "    decoder_outputs = activation(time_distributed_output)\n",
    "\n",
    "    # ==============\n",
    "    #   FULL MODEL\n",
    "    # ==============   \n",
    "    model= Model(inputs=[input_img, label_emb, input_caption], outputs=decoder_outputs)\n",
    "    rmsprop = RMSprop(lr=learning_rate, clipnorm=clip_norm)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model ## can add to this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_caption_input (InputLayer (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_label_input (InputLayer)  (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "caption_embeddings (Embedding)  (None, 30, 300)      7322700     image_caption_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 300)          22597784    image_input[0][0]                \n",
      "                                                                 image_label_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 30, 300), (N 721200      caption_embeddings[0][0]         \n",
      "                                                                 model_1[1][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 30, 24409)    7347109     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 30, 24409)    0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 37,988,793\n",
      "Trainable params: 16,186,009\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "meme_model = build_model(img_shape, vocab_size, embedding_size, maxlen, hidden_units, clip_norm, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of image data to store in cache\n",
    "base_fp = os.getcwd() + '/../memes/'\n",
    "image_dict = {}\n",
    "for name, fp in zip(captions.image.unique(), captions.file_path.unique()):\n",
    "    im = cv2.imread(base_fp + fp)\n",
    "    assert im is not None # check that the image has been read correctly\n",
    "    image_dict[name] = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(df, embeddings, word2idx, image_dict, batch_size=32, im_dim=(300, 300, 3)):\n",
    "    '''\n",
    "    Data generator\n",
    "    \n",
    "    Inputs:\n",
    "        df - Pandas dataframe with caption information\n",
    "        embeddings - matrix of embeddings to map from word indices\n",
    "        word2idx - matrix to convert words to indices (for image labels)\n",
    "        image_dict - dictionary containing the image pixel data, keys are labels\n",
    "        \n",
    "    Outputs: (batch of batch_size)\n",
    "        images - batch of pre-processed images\n",
    "        label_embs - batch of averaged image label embeddings\n",
    "        caption_inds - batch of caption indices \n",
    "        targets - batch of sequences of one-hot encoded sparse vocab vectors \n",
    "        \n",
    "    '''\n",
    "    while 1:  # needed for keras generator\n",
    "        # Shuffle data\n",
    "        df_new = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # Split into batches\n",
    "        split_ind = list(range(0, df_new.shape[0], batch_size))\n",
    "        batches = np.array_split(df_new, split_ind[1:])\n",
    "        for i, batch in enumerate(batches):\n",
    "            \n",
    "            # Prepare matrices to hold data\n",
    "            images = np.zeros((batch.shape[0], im_dim[0], im_dim[1], im_dim[2]))\n",
    "            label_embs = np.zeros((batch.shape[0], embeddings.shape[1]))\n",
    "            caption_inds = np.zeros((batch.shape[0], maxlen))\n",
    "            targets = np.zeros((batch.shape[0], maxlen, vocab_size))\n",
    "            \n",
    "            for j, (_, row) in enumerate(batch.iterrows()):\n",
    "                ### Prepare Image Data ###\n",
    "                im_data = image_dict[row.image]  # get image data for batch\n",
    "                \n",
    "                if im_data.shape != im_dim:  # resize if not 300 x 300\n",
    "                    im_data = cv2.resize(im_data, (im_dim[0], im_dim[1])) \n",
    "                    \n",
    "                im_data = im_data/255  # normalize\n",
    "                im_data = im_data.astype(np.float32)  # convert to single-precision\n",
    "                images[j] = im_data  # save the image\n",
    "                \n",
    "                ### Prepare Image Labels ###\n",
    "                im_label_words = row.image.split(' ')\n",
    "                im_label_ind = [word2idx[word] for word in im_label_words]\n",
    "                im_label_emb = [embeddings[ind] for ind in im_label_ind]\n",
    "                im_avg_emb = np.mean(im_label_emb, axis=0)  # average embedding\n",
    "                label_embs[j] = im_avg_emb\n",
    "                \n",
    "                ### Prepare Caption Indices ###\n",
    "                caption_ind = row.full_padded_caption\n",
    "                caption_inds[j] = caption_ind \n",
    "                \n",
    "                ### Prepare Target ###\n",
    "                target_ind = caption_ind[1:]  # target index is right shifted version of caption\n",
    "                #target_ind.append(1)  # add an extra eos\n",
    "                target_ind.append(0)  # add an extra eos\n",
    "                target = to_categorical(target_ind, num_classes=vocab_size)  # matrix of max_len x vocab size\n",
    "                targets[j] = target\n",
    "                \n",
    "                \n",
    "            yield [images, label_embs, caption_inds], targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  6/562 [..............................] - ETA: 1:17:32 - loss: 13.3088"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-108bf55734ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         verbose=1)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/109b/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/109b/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/109b/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/109b/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/109b/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/109b/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/109b/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = meme_model.fit_generator(data_gen(captions, embeddings, word2idx, image_dict, batch_size=32), \n",
    "                        steps_per_epoch=np.ceil(captions.shape[0]//batch_size), \n",
    "                        epochs=1,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, image, im_label, idx2word, greedy=True, k=3):\n",
    "    '''\n",
    "    Takes model predicted probabilities and converts to text\n",
    "    \n",
    "    inputs:\n",
    "        preds - vector of probability distributions over vocabulary\n",
    "        image - base image to predict for\n",
    "        im_label - image label associated with the base image (string)\n",
    "        idx2word - map of indices to words\n",
    "        \n",
    "    outputs:\n",
    "        caption - predicted caption text\n",
    "    '''\n",
    "    # preprocessing\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    im_label = np.expand_dims(imlabel_to_emb(im_label, embeddings, word2idx), axis=0)\n",
    "    caption = np.zeros((1, 30))\n",
    "    result = []\n",
    "    \n",
    "    for i in range(maxlen):\n",
    "        # make a prediction\n",
    "        preds = model.predict([image, im_label, caption])\n",
    "        \n",
    "        if greedy:  # implement greedy search\n",
    "            ind = np.argmax(preds[0, i])\n",
    "            \n",
    "        else:  # implement beam search\n",
    "            top_k_idx = np.argsort(preds[0, i])[-k:]\n",
    "            #ind = np.random.choice(top_k_idx) # unweighted\n",
    "            weights = sorted(preds[0, i])[-k:]\n",
    "            norm_weights = weights/np.sum(weights)\n",
    "            ind = np.random.choice(top_k_idx, p=norm_weights)\n",
    "            \n",
    "        caption[0, i] = ind\n",
    "        result.append(idx2word[ind])\n",
    "    return result\n",
    "\n",
    "    \n",
    "def imlabel_to_emb(label, embeddings, word2idx):\n",
    "    '''\n",
    "    Converts an image label to its average embedding\n",
    "    '''\n",
    "    words = label.split(' ')\n",
    "    word_inds = [word2idx[word] for word in words]\n",
    "    word_embs = [embeddings[ind] for ind in word_inds]\n",
    "    \n",
    "    avg_label_emb = np.mean(word_embs, axis=0)\n",
    "    return avg_label_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'mean',\n",
       " '<break>',\n",
       " 'you',\n",
       " 'mean',\n",
       " '<eos>',\n",
       " 'the',\n",
       " 'car',\n",
       " '<break>',\n",
       " 'i',\n",
       " 'will',\n",
       " 'find',\n",
       " 'you',\n",
       " 'and',\n",
       " 'only',\n",
       " 'you',\n",
       " '<eos>',\n",
       " 'and',\n",
       " 'i',\n",
       " 'will',\n",
       " 'kill',\n",
       " 'you',\n",
       " '<eos>',\n",
       " 'you',\n",
       " 'and',\n",
       " 'only',\n",
       " 'you',\n",
       " 'are',\n",
       " '<break>',\n",
       " 'i']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_image = list(image_dict.values())[0] # yo dawg meme\n",
    "im_label = 'yo man'\n",
    "inference(meme_model, pred_image, im_label, idx2word, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
