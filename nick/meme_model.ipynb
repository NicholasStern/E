{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meme Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, RepeatVector, Activation, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "from pickle_utils import pickle_load, pickle_dump\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check GPU\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "embeddings, idx2word, word2idx, captions = pickle_load(\"full_clean_processed_data.pkl\")\n",
    "captions.image = captions.image.apply(lambda x: x.strip(' '))\n",
    "#captions = captions.sample(frac=.5, random_state=19).reset_index(drop=True)  # Downsample for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "img_shape = (300,300,3)\n",
    "vocab_size = embeddings.shape[0]\n",
    "embedding_size = 300\n",
    "maxlen = 20                         # maximum length of the caption in hidden state\n",
    "batch_size = 32\n",
    "hidden_units = embedding_size       # length of word vectors i.e. embedding size\n",
    "\n",
    "# hyper params\n",
    "clip_norm = 1.0\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(img_shape, vocab_size, embedding_size, maxlen, hidden_units, clip_norm, learning_rate):\n",
    "    '''\n",
    "    =============\n",
    "      ENCODER\n",
    "    =============\n",
    "    Inputs: \n",
    "        1. Image (300, 300, 3)\n",
    "        2. GloVe-ed label embeddings (300,)\n",
    "        \n",
    "    Model:\n",
    "        3. Pretrained CNN with classification layer peeled off\n",
    "              - Output size: (2048,)\n",
    "        4. Concatenate with label embeding of size\n",
    "              - Output size: (2348,)\n",
    "        5. MLP: Dense layer with 300 nodes\n",
    "              - Output size: (300,) <-- This is the image embedding\n",
    "    '''\n",
    "\n",
    "    # 1. Image Input\n",
    "    input_img = keras.Input(shape=img_shape, name='image_input')\n",
    "    \n",
    "    # 2. Label Embedding\n",
    "    label_emb = keras.Input(shape=(300,), name='image_label_input')\n",
    "\n",
    "    # 3. Define Pretrained CNN - Inception V3\n",
    "    cnnModel = InceptionV3(weights='imagenet', \n",
    "                           include_top=False,        # this removes the final dense layer\n",
    "                           input_shape=img_shape, \n",
    "                           pooling = 'avg')\n",
    "\n",
    "    # freeze all convolutional InceptionV3 layers\n",
    "    for layer in cnnModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get image embedding <- this is a model output\n",
    "    image_emb = cnnModel(input_img)\n",
    "\n",
    "    # 4. Concatenate image embedding with label embedding\n",
    "    concat = keras.layers.Concatenate(axis=1)([image_emb, label_emb])\n",
    "    \n",
    "    # 5. MLP with 300 nodes\n",
    "    full_img_embedding = Dense(300, activation='relu')(concat)\n",
    "\n",
    "    # ==== ENCODER MODEL ====\n",
    "    encoder = keras.Model(inputs=[input_img, label_emb], outputs=full_img_embedding)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    =============\n",
    "      DECODER\n",
    "    =============\n",
    "    Inputs: \n",
    "        8. Caption (tokenized) (20,) <- limiting caption length to 20\n",
    "        9. LSTM hidden state from encoder\n",
    "        \n",
    "    Model:\n",
    "        10. Embedding layer that uses the GloVe embedding matrix, and is set to be trainable\n",
    "              - Output size: (20, 300)\n",
    "        11. LSTM\n",
    "              -  Output size: (20, 300)\n",
    "        12. Time Distributed layer to apply Dense layer to all the time step outputs\n",
    "              - Output size: (20, 40000)\n",
    "        13. Activation of softmax to get values between 0 and 1\n",
    "              - Output size: (20, 40000)\n",
    "    '''\n",
    "\n",
    "    # 8. Caption\n",
    "    input_caption = keras.Input(shape = (maxlen,), name='image_caption_input')\n",
    "    \n",
    "    # 9. Input for the LSTM hidden state and/or cell state\n",
    "    initial_state_LSTM = encoder([input_img, label_emb])\n",
    "    \n",
    "\n",
    "        \n",
    "    # 10. Embedding layer\n",
    "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_size,\n",
    "                                  input_length=maxlen, embeddings_regularizer = None,\n",
    "                                  weights = [embeddings], name = 'caption_embeddings', \n",
    "                                  trainable = True, mask_zero=True)\n",
    "    # 11. LSTM\n",
    "    decoder_LSTM = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    ## ===== Get embedding and LSTM outputs =====\n",
    "    decoder_embedding_outputs = decoder_embedding(input_caption)\n",
    "    decoder_LSTM_outputs, _ , _ = decoder_LSTM(decoder_embedding_outputs, \n",
    "                                          initial_state = [initial_state_LSTM,  # hidden state\n",
    "                                                           initial_state_LSTM]) # cell state\n",
    "    \n",
    "    # 12. Time Distributed Layer\n",
    "    time_distributed = TimeDistributed(Dense(vocab_size, name = 'timedistributed_1'))\n",
    "    \n",
    "    # 13. Softmax \n",
    "    activation = Activation('softmax')\n",
    "    \n",
    "    ## ===== Get time distributed and softmax output =====\n",
    "    time_distributed_output = time_distributed(decoder_LSTM_outputs)\n",
    "    decoder_outputs = activation(time_distributed_output)\n",
    "\n",
    "    # ==============\n",
    "    #   FULL MODEL\n",
    "    # ==============   \n",
    "    model= Model(inputs=[input_img, label_emb, input_caption], outputs=decoder_outputs)\n",
    "    rmsprop = RMSprop(lr=learning_rate, clipnorm=clip_norm, decay=.7)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model ## can add to this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_caption_input (InputLayer (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_label_input (InputLayer)  (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "caption_embeddings (Embedding)  (None, 20, 300)      11012700    image_caption_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 300)          22507484    image_input[0][0]                \n",
      "                                                                 image_label_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 20, 300), (N 721200      caption_embeddings[0][0]         \n",
      "                                                                 model_1[1][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 36709)    11049409    lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 20, 36709)    0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 45,290,793\n",
      "Trainable params: 23,488,009\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "meme_model = build_model(img_shape, vocab_size, embedding_size, maxlen, hidden_units, clip_norm, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of image data to store in cache\n",
    "base_fp = os.getcwd() + '/../memes/'\n",
    "image_dict = {}\n",
    "for name, fp in zip(captions.image.unique(), captions.file_path.unique()):\n",
    "    im = cv2.imread(base_fp + fp)\n",
    "    assert im is not None # check that the image has been read correctly\n",
    "    image_dict[name] = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(df, embeddings, word2idx, image_dict, batch_size=32, im_dim=(300, 300, 3)):\n",
    "    '''\n",
    "    Data generator\n",
    "    \n",
    "    Inputs:\n",
    "        df - Pandas dataframe with caption information\n",
    "        embeddings - matrix of embeddings to map from word indices\n",
    "        word2idx - matrix to convert words to indices (for image labels)\n",
    "        image_dict - dictionary containing the image pixel data, keys are labels\n",
    "        \n",
    "    Outputs: (batch of batch_size)\n",
    "        images - batch of pre-processed images\n",
    "        label_embs - batch of averaged image label embeddings\n",
    "        caption_inds - batch of caption indices \n",
    "        targets - batch of sequences of one-hot encoded sparse vocab vectors \n",
    "        \n",
    "    '''\n",
    "    while 1:  # needed for keras generator\n",
    "        # Shuffle data\n",
    "        df_new = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # Split into batches\n",
    "        split_ind = list(range(0, df_new.shape[0], batch_size))\n",
    "        batches = np.array_split(df_new, split_ind[1:])\n",
    "        for i, batch in enumerate(batches):\n",
    "            \n",
    "            # Prepare matrices to hold data\n",
    "            images = np.zeros((batch.shape[0], im_dim[0], im_dim[1], im_dim[2]))\n",
    "            label_embs = np.zeros((batch.shape[0], embeddings.shape[1]))\n",
    "            caption_inds = np.zeros((batch.shape[0], maxlen))\n",
    "            targets = np.zeros((batch.shape[0], maxlen, vocab_size))\n",
    "            \n",
    "            for j, (_, row) in enumerate(batch.iterrows()):\n",
    "                ### Prepare Image Data ###\n",
    "                im_data = image_dict[row.image]  # get image data for batch\n",
    "                \n",
    "                if im_data.shape != im_dim:  # resize if not 300 x 300\n",
    "                    im_data = cv2.resize(im_data, (im_dim[0], im_dim[1])) \n",
    "                    \n",
    "                im_data = im_data/255  # normalize\n",
    "                im_data = im_data.astype(np.float32)  # convert to single-precision\n",
    "                images[j] = im_data  # save the image\n",
    "                \n",
    "                ### Prepare Image Labels ###\n",
    "                im_label_words = row.image.split(' ')\n",
    "                im_label_ind = [word2idx[word] for word in im_label_words]\n",
    "                im_label_emb = [embeddings[ind] for ind in im_label_ind]\n",
    "                im_avg_emb = np.mean(im_label_emb, axis=0)  # average embedding\n",
    "                label_embs[j] = im_avg_emb\n",
    "                \n",
    "                ### Prepare Caption Indices ###\n",
    "                caption_ind = row.full_padded_caption\n",
    "                caption_inds[j] = caption_ind \n",
    "                \n",
    "                ### Prepare Target ###\n",
    "                target_ind = caption_ind[1:]  # target index is right shifted version of caption\n",
    "                #target_ind.append(1)  # add an extra eos\n",
    "                target_ind.append(0)  # add an extra eos\n",
    "                target = to_categorical(target_ind, num_classes=vocab_size)  # matrix of max_len x vocab size\n",
    "                targets[j] = target\n",
    "                \n",
    "                \n",
    "            yield [images, label_embs, caption_inds], targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aside test base image and associated captions\n",
    "test_image = captions.sample(1, random_state=19).image.values[0]\n",
    "test_ind = np.where(captions.image == test_image)[0]\n",
    "train_data = captions.drop(test_ind)\n",
    "test_data = captions.iloc[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "es = EarlyStopping(monitor='loss', mode='min', verbose=2, patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = meme_model.fit_generator(data_gen(train_data, embeddings, word2idx, image_dict, batch_size=32), \n",
    "                        steps_per_epoch=np.ceil(train_data.shape[0]//batch_size), \n",
    "                        epochs=10,\n",
    "                        verbose=1, \n",
    "                        callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if good\n",
    "#meme_model.save('meme_model_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, image, im_label, idx2word, greedy=True, k=3):\n",
    "    '''\n",
    "    Takes model predicted probabilities and converts to text\n",
    "    \n",
    "    inputs:\n",
    "        preds - vector of probability distributions over vocabulary\n",
    "        image - base image to predict for\n",
    "        im_label - image label associated with the base image (string)\n",
    "        idx2word - map of indices to words\n",
    "        \n",
    "    outputs:\n",
    "        caption - predicted caption text\n",
    "    '''\n",
    "    # preprocessing\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    im_label = np.expand_dims(imlabel_to_emb(im_label, embeddings, word2idx), axis=0)\n",
    "    caption = np.zeros((1, 20))\n",
    "    result = []\n",
    "    \n",
    "    for i in range(maxlen):\n",
    "        # make a prediction\n",
    "        preds = model.predict([image, im_label, caption])\n",
    "        \n",
    "        if greedy:  # implement greedy search\n",
    "            ind = np.argmax(preds[0, i])\n",
    "            \n",
    "        else:  # implement beam search\n",
    "            top_k_idx = np.argsort(preds[0, i])[-k:]\n",
    "            #ind = np.random.choice(top_k_idx) # unweighted\n",
    "            weights = sorted(preds[0, i])[-k:]\n",
    "            norm_weights = weights/np.sum(weights)\n",
    "            ind = np.random.choice(top_k_idx, p=norm_weights)\n",
    "            \n",
    "        caption[0, i] = ind\n",
    "        result.append(idx2word[ind])\n",
    "    return result\n",
    "\n",
    "    \n",
    "def imlabel_to_emb(label, embeddings, word2idx):\n",
    "    '''\n",
    "    Converts an image label to its average embedding\n",
    "    '''\n",
    "    words = label.split(' ')\n",
    "    word_inds = [word2idx[word] for word in words]\n",
    "    word_embs = [embeddings[ind] for ind in word_inds]\n",
    "    \n",
    "    avg_label_emb = np.mean(word_embs, axis=0)\n",
    "    return avg_label_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#meme_model.load('meme_model_v1') # load model\n",
    "\n",
    "filepath = os.getcwd() + '/../base_images/pavlos.png'\n",
    "pred_image = cv2.imread(filepath) # pavlos\n",
    "assert pred_image is not None  # make sure image gets read in\n",
    "# pred_image = image_dict[test_image]\n",
    "pred_image = cv2.resize(pred_image, (300, 300))\n",
    "plt.imshow(pred_image)\n",
    "plt.show()\n",
    "#im_label = test_image\n",
    "im_label = input()\n",
    "inference(meme_model, pred_image, im_label, idx2word, greedy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists a large compendium of techniques to evaluate the similarity between a machine generated caption and a human generated caption. Typically the similarity is computed using a **candidate sentence** generated by an ML algorithm and a **reference sentence** (or multiple) generated by a human. A few examples include:\n",
    "- **BLEU (2002)**\n",
    "    - At its core, BLEU is the precision of the candidate sentence, a.k.a, the proportion of words in the candidate sentence that also appear in the reference sentence. It extends to doing multiple n-gram comparisons and taking a weighted average. A more thorough description and example implementation in python can be found [here](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/). Extensions to this method penalize candidate sentences that are shorter than the reference sentence.  \n",
    "    \n",
    "    \n",
    "- **ROUGE (2004)**\n",
    "    - The recall of the candidate sentence. The proportion of words in the reference sentence that also appear in the candidate sentence. It's essentially the complement to BLEU, and they are often combined in a reported F1 score. Read more [here](https://stackoverflow.com/questions/38045290/text-summarization-evaluation-bleu-vs-rouge)\n",
    "    \n",
    "    \n",
    "- **METEOR (2005)**\n",
    "    - An extension to the precision/recall combo that algorithmically finds a mapping between the candidate text and the reference text, then uses that to compute the score. Wikipedia says \"Results have been presented which give correlation of up to 0.964 with human judgement at the corpus level, compared to BLEU's achievement of 0.817 on the same data set.\" This method also factors in synonyms. [source](https://en.wikipedia.org/wiki/METEOR)\n",
    "    \n",
    "    \n",
    "- **CIDEr (2015)**\n",
    "    - This method was developed specifically for image captioning, and extends the previous methods by doing a TF-IDF weighting before comparing the co-occurrence of n-grams between the candidate and reference sentence (actually a set of sentences typically). It is not always effective in situations where it adds disporportionate weight to unimportant words in a sentence that occur infrequently. [source](https://en.wikipedia.org/wiki/METEOR)\n",
    "    \n",
    "\n",
    "- **WMD (2015)**\n",
    "    - Uses word embeddings and something similar to Wasserstein distance to compute the discrepancy between a candidate sentence and a reference sentence. This snares the semantic similarities between two sentences that may not share commong words or even synonyms. [Here](https://vene.ro/blog/word-movers-distance-in-python.html) is a python blog post about it.\n",
    "    \n",
    "    \n",
    "- **SPICE (2016)**\n",
    "    - SPICE breaks down sentences into semantically meaningful components such as objects, attributes, and relation types. This graph structure is then used to create pairs of words that are semantically related, and computes and F1 score for the tuples between the candidate and the reference sentence(s). [This](https://aclweb.org/anthology/E17-1019) paper does a good job of summarizing this and all the above metrics.\n",
    "    \n",
    "    \n",
    "The paper linked [here](https://aclweb.org/anthology/E17-1019) does a phenomenal job of providing visual and tabular comparisons of each of the aforementioned metrics. The paper also examines their correlation with each other, concluding that the n-gram metrics (BLEU, ROUGE, METEOR, CIDEr) can complement the embedding (WMD) and graph-based (SPICE) ones. Here is a table and figure from the paper:\n",
    "\n",
    "![](nlp_metrics.png)\n",
    "\n",
    "\n",
    "We decided to use BLEU, mainly because it is easily transferrable to vocabularies with lots of slang words, and has a well documented and supported python implementation as a part of the NLTK library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-gram cumulative BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "smoother = SmoothingFunction()  \n",
    "reference = [['this', 'is', 'small', 'test']]  # Corpus of documents\n",
    "candidate = ['this', 'is', 'a', 'test']  # Candidate document\n",
    "score = sentence_bleu(reference, candidate, \n",
    "                      smoothing_function=smoother.method4, \n",
    "                      weights=(0.25, 0.25, 0.25, 0.25))  # ngram weights\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
