{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meme Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, RepeatVector, Activation, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "from pickle_utils import pickle_load, pickle_dump\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "embeddings, idx2word, word2idx, captions = pickle_load(\"small_processed_data.pkl\")\n",
    "captions.image = captions.image.apply(lambda x: x.strip(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "img_shape = (300,300,3)\n",
    "vocab_size = embeddings.shape[0]\n",
    "embedding_size = 300\n",
    "maxlen = 30                         # maximum length of the caption in hidden state\n",
    "batch_size = 32\n",
    "hidden_units = embedding_size       # length of word vectors i.e. embedding size\n",
    "\n",
    "# hyper params\n",
    "clip_norm = 1.0\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(img_shape, vocab_size, embedding_size, maxlen, hidden_units, clip_norm, learning_rate):\n",
    "    '''\n",
    "    =============\n",
    "      ENCODER\n",
    "    =============\n",
    "    Inputs: \n",
    "        1. Image (300, 300, 3)\n",
    "        2. GloVe-ed label embeddings (300,)\n",
    "        \n",
    "    Model:\n",
    "        3. Pretrained CNN with classification layer peeled off\n",
    "              - Output size: (2048,)\n",
    "        4. Concatenate with label embeding of size\n",
    "              - Output size: (2348,)\n",
    "        5. MLP: Dense layer with 300 nodes\n",
    "              - Output size: (300,) <-- This is the image embedding\n",
    "    '''\n",
    "\n",
    "    # 1. Image Input\n",
    "    input_img = keras.Input(shape=img_shape, name='image_input')\n",
    "    \n",
    "    # 2. Label Embedding\n",
    "    label_emb = keras.Input(shape=(300,), name='image_label_input')\n",
    "\n",
    "    # 3. Define Pretrained CNN - Inception V3\n",
    "    cnnModel = InceptionV3(weights='imagenet', \n",
    "                           include_top=False,        # this removes the final dense layer\n",
    "                           input_shape=img_shape, \n",
    "                           pooling = 'avg')\n",
    "\n",
    "    # freeze all convolutional InceptionV3 layers\n",
    "    for layer in cnnModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get image embedding <- this is a model output\n",
    "    image_emb = cnnModel(input_img)\n",
    "\n",
    "    # 4. Concatenate image embedding with label embedding\n",
    "    concat = keras.layers.Concatenate(axis=1)([image_emb, label_emb])\n",
    "    \n",
    "    # 5. MLP with 300 nodes\n",
    "    full_img_embedding = Dense(300, activation='relu')(concat)\n",
    "    \n",
    "    '''\n",
    "    =============\n",
    "    ATTENTION IMPLEMENTATION\n",
    "    =============\n",
    "        6. MLP with 300 nodes (so weights can be learned) w softmax activation to get importance probabilities\n",
    "        7. Multiply output of the full_img_embedding layer (Model part 3.) with \"probabilities\"\n",
    "    '''\n",
    "    \n",
    "    # 6. MLP with 300 nodes w softmax\n",
    "    softmax_encoder = Dense(300, activation = 'softmax', name = 'softmax_encoder')(full_img_embedding)\n",
    "    \n",
    "    # 7. Multiply layer i.e. probability weighted vector for naive impl of attention\n",
    "    attention_encoder = keras.layers.Multiply()([full_img_embedding, softmax_encoder])\n",
    "\n",
    "    # ==== ENCODER MODEL ====\n",
    "    encoder = keras.Model(inputs=[input_img, label_emb], outputs=attention_encoder)\n",
    "\n",
    "    '''\n",
    "    =============\n",
    "      DECODER\n",
    "    =============\n",
    "    Inputs: \n",
    "        8. Caption (tokenized) (20,) <- limiting caption length to 20\n",
    "        9. LSTM hidden state from encoder\n",
    "        \n",
    "    Model:\n",
    "        10. Embedding layer that uses the GloVe embedding matrix, and is set to be trainable\n",
    "              - Output size: (20, 300)\n",
    "        11. LSTM\n",
    "              -  Output size: (20, 300)\n",
    "        12. Time Distributed layer to apply Dense layer to all the time step outputs\n",
    "              - Output size: (20, 40000)\n",
    "        13. Activation of softmax to get values between 0 and 1\n",
    "              - Output size: (20, 40000)\n",
    "    '''\n",
    "\n",
    "    # 8. Caption\n",
    "    input_caption = keras.Input(shape = (maxlen,), name='image_caption_input')\n",
    "    \n",
    "    # 9. Input for the LSTM hidden state and/or cell state\n",
    "    initial_state_LSTM = encoder([input_img, label_emb])\n",
    "    \n",
    "\n",
    "        \n",
    "    # 10. Embedding layer\n",
    "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_size,\n",
    "                                  input_length=maxlen, embeddings_regularizer = None,\n",
    "                                  weights = [embeddings], name = 'caption_embeddings', \n",
    "                                  trainable = True, mask_zero=True)\n",
    "    # 11. LSTM\n",
    "    decoder_LSTM = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    ## ===== Get embedding and LSTM outputs =====\n",
    "    decoder_embedding_outputs = decoder_embedding(input_caption)\n",
    "    decoder_LSTM_outputs, _ , _ = decoder_LSTM(decoder_embedding_outputs, \n",
    "                                          initial_state = [initial_state_LSTM,  # hidden state\n",
    "                                                           initial_state_LSTM]) # cell state\n",
    "    \n",
    "    # 12. Time Distributed Layer\n",
    "    time_distributed = TimeDistributed(Dense(vocab_size, name = 'timedistributed_1'))\n",
    "    \n",
    "    # 13. Softmax \n",
    "    activation = Activation('softmax')\n",
    "    \n",
    "    ## ===== Get time distributed and softmax output =====\n",
    "    time_distributed_output = time_distributed(decoder_LSTM_outputs)\n",
    "    decoder_outputs = activation(time_distributed_output)\n",
    "\n",
    "    # ==============\n",
    "    #   FULL MODEL\n",
    "    # ==============   \n",
    "    model= Model(inputs=[input_img, label_emb, input_caption], outputs=decoder_outputs)\n",
    "    rmsprop = RMSprop(lr=learning_rate, clipnorm=clip_norm)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model ## can add to this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nicholasstern/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_caption_input (InputLayer (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_label_input (InputLayer)  (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "caption_embeddings (Embedding)  (None, 30, 300)      2226000     image_caption_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 300)          22597784    image_input[0][0]                \n",
      "                                                                 image_label_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 30, 300), (N 721200      caption_embeddings[0][0]         \n",
      "                                                                 model_1[1][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 30, 7420)     2233420     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 30, 7420)     0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 27,778,404\n",
      "Trainable params: 5,975,620\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "meme_model = build_model(img_shape, vocab_size, embedding_size, maxlen, hidden_units, clip_norm, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of image data to store in cache\n",
    "base_fp = os.getcwd() + '/../memes/'\n",
    "image_dict = {}\n",
    "for name, fp in zip(captions.image.unique(), captions.file_path.unique()):\n",
    "    im = cv2.imread(base_fp + fp)\n",
    "    assert im is not None # check that the image has been read correctly\n",
    "    image_dict[name] = im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(df, embeddings, word2idx, image_dict, batch_size=32, im_dim=(300, 300, 3)):\n",
    "    '''\n",
    "    Data generator\n",
    "    \n",
    "    Inputs:\n",
    "        df - Pandas dataframe with caption information\n",
    "        embeddings - matrix of embeddings to map from word indices\n",
    "        word2idx - matrix to convert words to indices (for image labels)\n",
    "        image_dict - dictionary containing the image pixel data, keys are labels\n",
    "        \n",
    "    Outputs: (batch of batch_size)\n",
    "        images - batch of pre-processed images\n",
    "        label_embs - batch of averaged image label embeddings\n",
    "        caption_inds - batch of caption indices \n",
    "        targets - batch of sequences of one-hot encoded sparse vocab vectors \n",
    "        \n",
    "    '''\n",
    "    while 1:  # needed for keras generator\n",
    "        # Shuffle data\n",
    "        df_new = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # Split into batches\n",
    "        split_ind = list(range(0, df_new.shape[0], batch_size))\n",
    "        batches = np.array_split(df_new, split_ind[1:])\n",
    "        for i, batch in enumerate(batches):\n",
    "            \n",
    "            # Prepare matrices to hold data\n",
    "            images = np.zeros((batch.shape[0], im_dim[0], im_dim[1], im_dim[2]))\n",
    "            label_embs = np.zeros((batch.shape[0], embeddings.shape[1]))\n",
    "            caption_inds = np.zeros((batch.shape[0], maxlen))\n",
    "            targets = np.zeros((batch.shape[0], maxlen, vocab_size))\n",
    "            \n",
    "            for j, (_, row) in enumerate(batch.iterrows()):\n",
    "                ### Prepare Image Data ###\n",
    "                im_data = image_dict[row.image]  # get image data for batch\n",
    "                \n",
    "                if im_data.shape != im_dim:  # resize if not 300 x 300\n",
    "                    im_data = cv2.resize(im_data, (im_dim[0], im_dim[1])) \n",
    "                    \n",
    "                im_data = im_data/255  # normalize\n",
    "                im_data = im_data.astype(np.float32)  # convert to single-precision\n",
    "                images[j] = im_data  # save the image\n",
    "                \n",
    "                ### Prepare Image Labels ###\n",
    "                im_label_words = row.image.split(' ')\n",
    "                im_label_ind = [word2idx[word] for word in im_label_words]\n",
    "                im_label_emb = [embeddings[ind] for ind in im_label_ind]\n",
    "                im_avg_emb = np.mean(im_label_emb, axis=0)  # average embedding\n",
    "                label_embs[j] = im_avg_emb\n",
    "                \n",
    "                ### Prepare Caption Indices ###\n",
    "                caption_ind = row.full_padded_caption\n",
    "                caption_inds[j] = caption_ind \n",
    "                \n",
    "                ### Prepare Target ###\n",
    "                target_ind = caption_ind[1:]  # target index is right shifted version of caption\n",
    "                #target_ind.append(1)  # add an extra eos\n",
    "                target_ind.append(0)  # add an extra eos\n",
    "                target = to_categorical(target_ind, num_classes=vocab_size)  # matrix of max_len x vocab size\n",
    "                targets[j] = target\n",
    "                \n",
    "                \n",
    "            yield [images, label_embs, caption_inds], targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nicholasstern/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/nicholasstern/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      "115/115 [==============================] - 779s 7s/step - loss: 10.5371\n"
     ]
    }
   ],
   "source": [
    "history = meme_model.fit_generator(data_gen(captions, embeddings, word2idx, image_dict, batch_size=32), \n",
    "                        steps_per_epoch=np.ceil(captions.shape[0]//batch_size), \n",
    "                        epochs=1,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, image, im_label, idx2word, greedy=True, k=3):\n",
    "    '''\n",
    "    Takes model predicted probabilities and converts to text\n",
    "    \n",
    "    inputs:\n",
    "        preds - vector of probability distributions over vocabulary\n",
    "        image - base image to predict for\n",
    "        im_label - image label associated with the base image (string)\n",
    "        idx2word - map of indices to words\n",
    "        \n",
    "    outputs:\n",
    "        caption - predicted caption text\n",
    "    '''\n",
    "    # preprocessing\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    im_label = np.expand_dims(imlabel_to_emb(im_label, embeddings, word2idx), axis=0)\n",
    "    caption = np.zeros((1, 30))\n",
    "    result = []\n",
    "    \n",
    "    for i in range(maxlen):\n",
    "        # make a prediction\n",
    "        preds = model.predict([image, im_label, caption])\n",
    "        \n",
    "        if greedy:  # implement greedy search\n",
    "            ind = np.argmax(preds[0, i])\n",
    "            \n",
    "        else:  # implement beam search\n",
    "            top_k_idx = np.argsort(preds[0, i])[-k:]\n",
    "            #ind = np.random.choice(top_k_idx) # unweighted\n",
    "            weights = sorted(preds[0, i])[-k:]\n",
    "            norm_weights = weights/np.sum(weights)\n",
    "            ind = np.random.choice(top_k_idx, p=norm_weights)\n",
    "            \n",
    "        caption[0, i] = ind\n",
    "        result.append(idx2word[ind])\n",
    "    return result\n",
    "\n",
    "    \n",
    "def imlabel_to_emb(label, embeddings, word2idx):\n",
    "    '''\n",
    "    Converts an image label to its average embedding\n",
    "    '''\n",
    "    words = label.split(' ')\n",
    "    word_inds = [word2idx[word] for word in words]\n",
    "    word_embs = [embeddings[ind] for ind in word_inds]\n",
    "    \n",
    "    avg_label_emb = np.mean(word_embs, axis=0)\n",
    "    return avg_label_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'mean',\n",
       " '<break>',\n",
       " 'you',\n",
       " 'mean',\n",
       " '<eos>',\n",
       " 'the',\n",
       " 'car',\n",
       " '<break>',\n",
       " 'i',\n",
       " 'will',\n",
       " 'find',\n",
       " 'you',\n",
       " 'and',\n",
       " 'only',\n",
       " 'you',\n",
       " '<eos>',\n",
       " 'and',\n",
       " 'i',\n",
       " 'will',\n",
       " 'kill',\n",
       " 'you',\n",
       " '<eos>',\n",
       " 'you',\n",
       " 'and',\n",
       " 'only',\n",
       " 'you',\n",
       " 'are',\n",
       " '<break>',\n",
       " 'i']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_image = list(image_dict.values())[0] # yo dawg meme\n",
    "im_label = 'yo man'\n",
    "inference(meme_model, pred_image, im_label, idx2word, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
