{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the Caption Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3pt'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists a large compendium of techniques to evaluate the similarity between a machine generated caption and a human generated caption. Typically the similarity is computed using a **candidate sentence** generated by an ML algorithm and a **reference sentence** (or multiple) generated by a human. A few examples include:\n",
    "- **BLEU (2002)**\n",
    "    - At its core, BLEU is the precision of the candidate sentence, a.k.a, the proportion of words in the candidate sentence that also appear in the reference sentence. It extends to doing multiple n-gram comparisons and taking a weighted average. A more thorough description and example implementation in python can be found [here](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/). Extensions to this method penalize candidate sentences that are shorter than the reference sentence.  \n",
    "    \n",
    "    \n",
    "- **ROUGE (2004)**\n",
    "    - The recall of the candidate sentence. The proportion of words in the reference sentence that also appear in the candidate sentence. It's essentially the complement to BLEU, and they are often combined in a reported F1 score. Read more [here](https://stackoverflow.com/questions/38045290/text-summarization-evaluation-bleu-vs-rouge)\n",
    "    \n",
    "    \n",
    "- **METEOR (2005)**\n",
    "    - An extension to the precision/recall combo that algorithmically finds a mapping between the candidate text and the reference text, then uses that to compute the score. Wikipedia says \"Results have been presented which give correlation of up to 0.964 with human judgement at the corpus level, compared to BLEU's achievement of 0.817 on the same data set.\" This method also factors in synonyms. [source](https://en.wikipedia.org/wiki/METEOR)\n",
    "    \n",
    "    \n",
    "- **CIDEr (2015)**\n",
    "    - This method was developed specifically for image captioning, and extends the previous methods by doing a TF-IDF weighting before comparing the co-occurrence of n-grams between the candidate and reference sentence (actually a set of sentences typically). It is not always effective in situations where it adds disporportionate weight to unimportant words in a sentence that occur infrequently. [source](https://en.wikipedia.org/wiki/METEOR)\n",
    "    \n",
    "\n",
    "- **WMD (2015)**\n",
    "    - Uses word embeddings and something similar to Wasserstein distance to compute the discrepancy between a candidate sentence and a reference sentence. This snares the semantic similarities between two sentences that may not share commong words or even synonyms. [Here](https://vene.ro/blog/word-movers-distance-in-python.html) is a python blog post about it.\n",
    "    \n",
    "    \n",
    "- **SPICE (2016)**\n",
    "    - SPICE breaks down sentences into semantically meaningful components such as objects, attributes, and relation types. This graph structure is then used to create pairs of words that are semantically related, and computes and F1 score for the tuples between the candidate and the reference sentence(s). [This](https://aclweb.org/anthology/E17-1019) paper does a good job of summarizing this and all the above metrics.\n",
    "    \n",
    "    \n",
    "The paper linked [here](https://aclweb.org/anthology/E17-1019) does a phenomenal job of providing visual and tabular comparisons of each of the aforementioned metrics. The paper also examines their correlation with each other, concluding that the n-gram metrics (BLEU, ROUGE, METEOR, CIDEr) can complement the embedding (WMD) and graph-based (SPICE) ones. Here is a table and figure from the paper:\n",
    "\n",
    "![](nlp_metrics.png)\n",
    "\n",
    "\n",
    "Now, I want to try  Meteor and WMD in combination to capture both lexical and semantic similarities, but will go through each to see how difficult it is to get them working in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2866227639866161\n"
     ]
    }
   ],
   "source": [
    "# 4-gram cumulative BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "smoother = SmoothingFunction()  \n",
    "reference = [['this', 'is', 'small', 'test']]  # Corpus of documents\n",
    "candidate = ['this', 'is', 'a', 'test']  # Candidate document\n",
    "score = sentence_bleu(reference, candidate, \n",
    "                      smoothing_function=smoother.method4, \n",
    "                      weights=(0.25, 0.25, 0.25, 0.25))  # ngram weights\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.1499999982, 'p': 0.08333333333333333, 'r': 0.75},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.05616438356152923, 'p': 0.05555555555555555, 'r': 0.5}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install rouge\n",
    "from rouge import Rouge \n",
    "\n",
    "reference = 'this is small test'  # Corpus of documents\n",
    "candidate = 'this is a test'  # Candidate document\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METEOR Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow installation instructions [here](https://github.com/Maluuba/nlg-eval), then run ```python setup.py install```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlgeval import compute_individual_metrics\n",
    "reference = ['this is small test']  # Corpus of documents\n",
    "candidate = 'this is a test'  # Candidate document\n",
    "metrics_dict = compute_individual_metrics(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bleu_1': 0.7499999996250004,\n",
       " 'Bleu_2': 0.4999999997291671,\n",
       " 'Bleu_3': 4.999999996944452e-06,\n",
       " 'Bleu_4': 1.8803015450937985e-08,\n",
       " 'METEOR': 0.2900878266954308,\n",
       " 'ROUGE_L': 0.75,\n",
       " 'CIDEr': 0.0,\n",
       " 'SkipThoughtCS': 0.84144896,\n",
       " 'EmbeddingAverageCosineSimilairty': 0.950262,\n",
       " 'VectorExtremaCosineSimilarity': 0.833967,\n",
       " 'GreedyMatchingScore': 0.891991}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMD Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try [this](https://github.com/RaRe-Technologies/gensim/blob/c971411c09773488dbdd899754537c0d1a9fce50/docs/notebooks/WMD_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determined the computation overhead was too much"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
